<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="FRODO a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps.">
    <meta name="keywords"
        content="Chain of thought, Feedback, ChatGPT, Refinement, faithful reasoning, LLM, GPT4">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Making Reasoning Matter </title>

    
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/themes/prism-coy.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/plugins/line-numbers/prism-line-numbers.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="https://d3js.org/d3.v4.js"></script>
    <style>
        .long_opt {
            max-width: 100%;
            white-space: normal;
            text-overflow: ellipsis;
        }
    </style>
    <script>
        // toggle the visibility of the element with id "element_id"
        function toggle_visibility(element_id) {
            var e = document.getElementById(element_id);
            if (e.style.display == 'block')
                e.style.display = 'none';
            else
                e.style.display = 'block';
        }

    </script>
    <!-- <script>
        (function () {
            ("#outputs").load("./static/outputs.html");
        });
    </script> -->

    <!--font awesome-->
    <script src="https://kit.fontawesome.com/yourcode.js" crossorigin="anonymous"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Making Reasoning Matter: <br>
                            Measuring and Improving Faithfulness of Chain-of-Thought Reasoning
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://debjitpaul.github.io/">Debjit Paul</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://dlab.epfl.ch/people/west/">Robert West</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://atcbosselut.github.io/">Antoine Bosselut</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.epfl.ch/boi.faltings?lang=en">Boi Faltings</a>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">EPFL</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2402.13950"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/debjitpaul/causal_CoT"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <div class="content has-text-justified is-size-4">
                    <p> Chain-of-thought (CoT) reasoning techniques have been shown to improve the performance of large language models (LLMs) by generating step-by-step reasoning traces before generating a final answer. In standard CoT, LLMs can generate plausible explanations, with the final answer not necessarily guaranteed to follow the reasoning chain or imply a causal relation between the reasoning chain and the modelâ€™s outcome. 
                        We use the Causal Mediation Analysis method to measure and interpret the contribution of a reasoning chain (mediator) to the final answer (observed output). Our causal study highlights two issues: (i) LLMs can generate an unfaithful and implausible reasoning chain, and (ii) LLMs are inconsistent when reasoning over their own generated reasoning traces.
                        Finally, we introduce FRODO, a novel framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective.</p>
                </div>
                <figure class="image is-5by5">
                    <img id="animatedGif" src="static/image/figure_1.png">
                  </figure>
                <h2 class="subtitle has-text-centered">
                    <span class="pal">
                        Figure 1. An example of our proposed causal analysis to measure the faithfulness of the final output with respect to the CoT generated by the model. We perturbed CoT rationales and studied the causal impact on the model's behaviour.
                </h2>

            </div>

        </div>
    </section>


    <hr>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Causal Mediation analysis</h2>
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <br>
                            <figure class="image">
                                <img src="static/image/CSM.png">
                            </figure>
                            <figcaption> Figure 2. Causal graph for natural language reasoning, where we model P(Y| do(x)). X<sub>0</sub> = original reasoning problem, X<sub>1</sub> = intervened reasoning problem, R<sub>0</sub> = reasoning steps for X<sub>0</sub> reasoning problem, R<sub>1</sub> = reasoning steps for X<sub>1</sub>, Y = output, and M = model parameters</figcaption>
                        </div>
                    </div>
                    <br> <br>

                    <div class="columns">
                        <div class="column has-text-justified">
                            <h3 class="title is-4">Reasoning Chain as Mediator</h3>
                            <ul>
                                <li>In this work, we view the reasoning process as a causal graph, framing the input (reasoning problem) X and the output Y as random variables and the reasoning steps as mediator variable R.  
                                We use mediation analysis to interpret the role of reasoning steps as mediators between model inputs and model outputs. Please note that there are two mediator variables, model parameters M and reasoning steps R (see above Figure 2(b)). 
                                However, in this work, we are interested in understanding the indirect effect of R on the output Y; hence, we assume model parameters M to be fixed. 
                                Let X<sub>0</sub> denote the initial reasoning problem, R<sub>0</sub> as the reasoning chain given X<sub>0</sub>, and Y<sub>00</sub> denote the potential outcome if the treatment variable and mediator variable are X<sub>0</sub> and R<sub>0</sub>, respectively. 
                                Whereas Y<sub>01</sub> denotes the possible outcome when treatment is set to X<sub>0</sub> and R<sub>1</sub> is the reasoning chain for an intervened reasoning problem (see Figure 2(d)). 
                                </li>

                            </ul>
                            <br><br><br>

                            <h3 class="title is-4">Analysis Outcomes</h3>
                            <ul>
                                <li>Our study suggests that vanilla LMs (<20B) (in a zero-shot setting) are systematically unfaithful and consistently fail to reason over the mediator.
                                    In general, our experiments show a large variation in the causal effects of COT in the final answer depending on the tasks. 
                                    Models that are instruction-tuned or trained on the chain of thought during the pre-training phase have a better indirect effect across different reasoning tasks, suggesting that fine-tuning on CoT can make the model more faithful. Interestingly, we observe that there is an inverse scaling for certain tasks. 
                                    In our case, the indirect effect is worsening with increasingly capable models, indicating that smaller models might be more effective in faithful reasoning. 
                                </li>
                            </ul>   
                        </div>
                    </div>
                    <br><br>
                    <br>
                    <figure class="image is-8by6">
                    <img id="animatedGif" src="static/image/Frodo.png">
                  </figure>
                <h2 class="subtitle has-text-centered">
                    <span class="pal">
                        An overview of the FRODO framework. It comprises two modules: (i) Inference Module and (ii) Reasoner Module. 
                </h2>

                <div class="columns">
                        <div class="column has-text-justified">
                            <h3 class="title is-4">FRODO</h3>
                            We propose a new framework, FRODO, that focuses on tailoring small-sized LMs (<5B parameters) to be strong rationalizers and perform reasoning faithfully over the rationales. FRODO aims to improve the synergy between the reasoning chain and the final answer.
                            <ul>
                                <li><b>Inference Module</b>: The first module tailors small-sized LMs to generate correct reasoning chains (inference module), while the second module takes the reasoning chains as input and faithfully reasons over them to arrive at the correct answer (reasoning module). 
                                To learn to generate correct reasoning chains, we use the DPO algorithm, which enables the model to prefer correct reasoning chains over counterfactual ones with implicit feedback. Instead of relying on human labelling, we obtain preference data by prompting LLMs to generate correct and counterfactual reasoning chains.    
                                </li>
                                <br>
                                <li><b>Reasoner Module</b>: In this module, we train another small-sized LM to improve the causal effect between the reasoning chain and the final answer using a counterfactual and causal preference ranking objective.
                                </li>

                            </ul>
                        </div>
                    </div>
                    <br><br>
                </div>
            </div>
        </div>
    </section>


    <hr>
    <section class="section" id="results">
        <div class="container is-max-desktop">
            <div class="columns is-centered ">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Results summary</h2>

                    <h2 class="title is-4">Causal Analysis</h2>
                    <p>
                        Table 1 shows the results of causal mediation analysis for 11 different LMs. 
                        In these experiments, we examined the causal behaviour using reasoning chains generated by GPT-4 (controlled setting). 
                        We find that in-context learning and instruction-tuning improve the indirect effect over models trained only with language modelling objectives (e.g., LLaMA and Mistral), indicating that these methods help the model align better with the reasoning chains. 
                        We observe that models trained with RLHF objective (ChatGPT, Llama-2-7B-Chat) have a more direct effect than an indirect effect, suggesting that training on human feedback might have disincentive faithful reasoning. 
                        Interestingly, we observe that none of the models has high indirect or direct effects on the causal understanding task. 
                        One intuitive reason is that the causal understanding task is challenging, and the model's (<10B) performance is nearly random; hence, the effects are not strong. 
                        Overall, we observe that LLMs are inconsistent in faithfully performing reasoning over the CoT.

                    </p>
                    </br></br>
                    <!--row1, colored objects-->

                    <figure class="image is-64by64">
                        <img src="static/image/analysis_result.png">
                        <br>
                        <caption></strong>Table 1. <b>Causal Effects of CoT. The reported results are zero-shot performance. CIE: Controlled Indirect Effect, CDE: Controlled Direct Effect.</b></caption>
                    </figure>
                    <hr>

                    <p>
                        Table 2 shows the zero-shot performance of the ChatGPT and GPT-4 models. We observe that for StrategyQA and Causal Understanding tasks, 
                        GPT-4 has a higher natural indirect effect than ChatGPT, suggesting that it is able to better reason over the reasoning steps for these tasks. 
                        However, for mathematical reasoning (GSM8K), ChatGPT has a better indirect effect. Qualitatively, we find that for mathematical reasoning, when we provide intervened reasoning steps, GPT-4 considers them incorrect and continues to generate correct reasoning steps. 
                        This results in a lower indirect effect score. 
                        Moreover, GPT-4 exhibits a more pronounced direct effect than Chat-GPT, suggesting that its outputs are more causally sensitive to reasoning problems.
                    </p>
                    </br></br>
                    <!--row1, colored objects-->

                    <figure class="image is-64by64">
                        <img src="static/image/result_2.png">
                        <br>
                        <caption></strong>Table 2. <b>Causal Effects of generated CoT and reasoning problems on the outputs, with both Natural Indirect Effect (NIE) and Natural Direct Effect (NDE).</b></caption>
                    </figure>
                    <hr>

                    <h2 class="title is-4">FRODO results</h2>

                    <div>
                        We evaluate FRODO on four reasoning tasks (Quarel, StrategyQA, OpenBookQA, QASC) using multiple model backbones of different scales and demonstrate that \ourmodel achieves an absolute accuracy improvement of 2%~3% over baselines trained with standard supervised fine-tuning or chain-of-thought distilled methods.
                        FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets.

                    </div>

                    <!--row2, math reasoning-->
                    <figure class="image is-64by64">
                        <img src="static/image/result_1.png">
                        <caption><strong><span class="pal"></strong> Table 3. <b>Performance of small-sized LMs on four different reasoning tasks. The base models are T5-large (770M) and Pink Color: {T5-3B} (3B). We report accuracy (%).</b> </caption>
                    </figure>
            </div>
        </div>
    </section>
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
@misc{debjit2024frodo,
    title={Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning}, 
    author={Debjit Paul, Robert West, Antoine Bosselut and Boi Faltings}
    year={2024},
    eprint={},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
            </code></pre>
        </div>
    </section>

    <hr>



















    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            Template adapted from <a href="http://nerfies.github.io/">Nerfies</a> by Keunhong Park et
                            al.
                            and uses <a href="https://bulma.io/">Bulma</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
